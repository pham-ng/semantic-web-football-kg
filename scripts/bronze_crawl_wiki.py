#!/usr/bin/env python3
import os
import json
import re
import time
import argparse
from urllib.parse import quote
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
from collections import deque
from typing import Set

OUTPUT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../bronze/raw"))
DONE_FILE = os.path.join(OUTPUT_DIR, "..", "done.txt")
ERROR_FILE = os.path.join(OUTPUT_DIR, "..", "error.txt")

WIKI_API = "https://vi.wikipedia.org/w/api.php"
REST_SUMMARY = "https://vi.wikipedia.org/api/rest_v1/page/summary/"

# Máº·c Ä‘á»‹nh (cÃ³ thá»ƒ override qua CLI)
DEFAULT_MAX_DEPTH = 3
DEFAULT_MAX_PAGES = 25000
DEFAULT_DELAY = 0.3
DEFAULT_FULL = True  # False: chá»‰ intro, True: toÃ n bá»™ pháº§n vÄƒn báº£n
DEFAULT_MODE = "extract"  # extract | wikitext | html

# PhÃ¢n loáº¡i lá»—i
RETRYABLE_STATUS = {403, 408, 429, 500, 502, 503, 504, 520, 522}
PERMANENT_STATUS = {400, 401, 404, 410}

# Tá»« khÃ³a Ä‘á»ƒ lá»c theo chá»§ Ä‘á» bÃ³ng Ä‘Ã¡ (Ä‘Æ¡n giáº£n)
FOOTBALL_KEYWORDS = [
    "bÃ³ng Ä‘Ã¡",
    "bong da",
    "fc",
    "v.league",
    "vleague",
    "cup",
    "futsal",
    "giáº£i vÃ´ Ä‘á»‹ch",
    "Ä‘á»™i tuyá»ƒn",
    "huáº¥n luyá»‡n viÃªn",
    "sÃ¢n váº­n Ä‘á»™ng",
    "aff",
    "sea games",
    "cáº§u thá»§",
]

TOPICS = [
    "Äá»™i tuyá»ƒn bÃ³ng Ä‘Ã¡ quá»‘c gia Viá»‡t Nam",
    "Cáº§u thá»§ bÃ³ng Ä‘Ã¡ Viá»‡t Nam",
    "CÃ¢u láº¡c bá»™ bÃ³ng Ä‘Ã¡ Viá»‡t Nam",
    "Äá»™i tuyá»ƒn bÃ³ng Ä‘Ã¡ ná»¯ quá»‘c gia Viá»‡t Nam",
    "Äá»™i tuyá»ƒn bÃ³ng Ä‘Ã¡ U23 Viá»‡t Nam",
    "V.League 1",
    "CÃºp Quá»‘c gia Viá»‡t Nam",
    "AFF Cup",
    "SEA Games bÃ³ng Ä‘Ã¡",
    "Giáº£i vÃ´ Ä‘á»‹ch bÃ³ng Ä‘Ã¡ ÄÃ´ng Nam Ã",
    "HÃ  Ná»™i FC",
    "HoÃ ng Anh Gia Lai",
    "CÃ´ng PhÆ°á»£ng",
    "Nguyá»…n Quang Háº£i",
    "LÃª CÃ´ng Vinh",
    "Nguyá»…n CÃ´ng PhÆ°á»£ng",
    "LÆ°Æ¡ng XuÃ¢n TrÆ°á»ng",
    "BÃ³ng Ä‘Ã¡ tráº» Viá»‡t Nam",
    "BÃ³ng Ä‘Ã¡ ná»¯ Viá»‡t Nam",
    "Futsal Viá»‡t Nam",
    "Lá»‹ch sá»­ bÃ³ng Ä‘Ã¡ Viá»‡t Nam",
    "LiÃªn Ä‘oÃ n bÃ³ng Ä‘Ã¡ Viá»‡t Nam",
    "SÃ¢n váº­n Ä‘á»™ng Má»¹ ÄÃ¬nh",
    "Huáº¥n luyá»‡n viÃªn Park Hang-seo",
    "Giáº£i háº¡ng Nháº¥t Quá»‘c gia Viá»‡t Nam",
]


def get_session() -> requests.Session:
    session = requests.Session()
    headers = {
        "User-Agent": "KG-Football-Bot/0.1 (+https://kg-football.vn; contact=admin@kg-football.vn)",
        "Accept": "application/json",
    }
    session.headers.update(headers)
    retry = Retry(
        total=5,
        backoff_factor=1.0,
        status_forcelist=list(RETRYABLE_STATUS),
        allowed_methods=["GET"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session


SESSION = get_session()


def fetch_page(title: str, *, mode: str = DEFAULT_MODE, full: bool = DEFAULT_FULL):
    """Tráº£ vá» dá»¯ liá»‡u trang theo mode:
    - extract: dÃ¹ng extracts (intro hoáº·c full text)
    - wikitext: dÃ¹ng revisions API Ä‘á»ƒ láº¥y wikitext Ä‘áº§y Ä‘á»§
    - html: dÃ¹ng action=parse&prop=text Ä‘á»ƒ láº¥y HTML render
    """
    if mode == "wikitext":
        params = {
            "action": "query",
            "prop": "revisions|info",
            "titles": title,
            "rvprop": "content",
            "rvslots": "main",
            "format": "json",
            "formatversion": 2,
            "utf8": 1,
            "maxlag": 5,
            "redirects": 1,
            "inprop": "url",
        }
        r = SESSION.get(WIKI_API, params=params, timeout=30)
        r.raise_for_status()
        j = r.json()
        return {"mode": mode, "query": j.get("query", {})}, r.status_code

    if mode == "html":
        params = {
            "action": "parse",
            "page": title,
            "prop": "text|links|templates",
            "format": "json",
            "utf8": 1,
            "maxlag": 5,
            "redirects": 1,
        }
        r = SESSION.get(WIKI_API, params=params, timeout=30)
        r.raise_for_status()
        j = r.json()
        return {"mode": mode, "parse": j.get("parse", {})}, r.status_code

    # default: extract
    params = {
        "action": "query",
        "prop": "extracts|pageimages|info",
        "explaintext": True,
        "inprop": "url",
        "format": "json",
        "formatversion": 2,
        "utf8": 1,
        "maxlag": 5,
        "titles": title,
        "pithumbsize": 300,
        "redirects": 1,
    }
    if not full:
        params["exintro"] = True

    r = SESSION.get(WIKI_API, params=params, timeout=30)
    if r.status_code in (403, 429):
        # Fallback sang REST summary API (chá»‰ tÃ³m táº¯t)
        ru = REST_SUMMARY + quote(title)
        r2 = SESSION.get(ru, timeout=30)
        r2.raise_for_status()
        j = r2.json()
        page = {
            "title": j.get("title", title),
            "extract": j.get("extract", ""),
            "content_urls": j.get("content_urls", {}),
            "thumbnail": j.get("thumbnail", {}),
        }
        return {"mode": mode, "query": {"pages": [page]}}, r.status_code
    r.raise_for_status()
    j = r.json()
    return {"mode": mode, "query": j.get("query", {})}, r.status_code


def fetch_links(title: str, delay: float):
    links = []
    params = {
        "action": "query",
        "prop": "links",
        "titles": title,
        "plnamespace": 0,
        "pllimit": 100,
        "format": "json",
        "formatversion": 2,
        "utf8": 1,
        "maxlag": 5,
    }
    cont = None
    while True:
        if cont:
            params.update({"plcontinue": cont})
        r = SESSION.get(WIKI_API, params=params, timeout=30)
        if r.status_code in (403, 429):
            time.sleep(max(1.0, delay))
            continue
        r.raise_for_status()
        j = r.json()
        pages = j.get("query", {}).get("pages", [])
        if pages:
            for p in pages:
                for l in p.get("links", []):
                    t = l.get("title")
                    if t:
                        links.append(t)
        cont = j.get("continue", {}).get("plcontinue")
        if not cont:
            break
    return links


def slugify(name: str) -> str:
    s = re.sub(r"\s+", "_", name.strip())
    s = s.replace("/", "_")
    return quote(s)


def is_footballish(title: str) -> bool:
    lower = title.lower()
    return any(k in lower for k in FOOTBALL_KEYWORDS)


def save_page_json(title: str, data: dict):
    fn = os.path.join(OUTPUT_DIR, f"wiki_{slugify(title)}.json")
    with open(fn, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return fn


def load_permanent_errors() -> Set[str]:
    skip = set()
    if os.path.exists(ERROR_FILE):
        with open(ERROR_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split("\t")
                # format: title\tstatus\tretryable\ttimestamp\tmessage
                if len(parts) >= 3:
                    title, _, retryable = parts[0], parts[1], parts[2]
                    if retryable == "0":
                        skip.add(title)
    return skip


def append_error(title: str, status: int, retryable: bool, message: str):
    ts = int(time.time())
    with open(ERROR_FILE, "a", encoding="utf-8") as f:
        f.write(f"{title}\t{status}\t{1 if retryable else 0}\t{ts}\t{message}\n")


def run(max_depth: int, max_pages: int, delay: float, full: bool, mode: str):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    done = set()
    if os.path.exists(DONE_FILE):
        with open(DONE_FILE, "r", encoding="utf-8") as f:
            done = set(line.strip() for line in f if line.strip())

    permanent_error_skip = load_permanent_errors()

    results = []
    queue = deque([(t, 0) for t in TOPICS])
    visited = set(done) | set(permanent_error_skip)

    with tqdm(total=max_pages, desc="Crawling Wikipedia BFS") as pbar:
        while queue and (len(results) + len(done)) < max_pages:
            title, depth = queue.popleft()
            if title in visited:
                continue
            visited.add(title)
            try:
                data, status = fetch_page(title, mode=mode, full=full)
                results.append({"title": title, "data": data})
                fn = save_page_json(title, data)
                with open(DONE_FILE, "a", encoding="utf-8") as f:
                    f.write(title + "\n")
                pbar.update(1)

                if depth < max_depth:
                    try:
                        for lt in fetch_links(title, delay):
                            if lt not in visited and is_footballish(lt):
                                queue.append((lt, depth + 1))
                    except Exception as le:
                        print(f"âš ï¸ Link expand error for {title}: {le}")

            except requests.HTTPError as he:
                status = getattr(he.response, "status_code", None) or 0
                retryable = status in RETRYABLE_STATUS
                print(f"âŒ Error fetching {title}: {status} retryable={retryable}")
                append_error(title, status, retryable, str(he))
            except Exception as e:
                print(f"âŒ Error fetching {title}: {e}")
                append_error(title, 0, True, str(e))
            finally:
                time.sleep(delay)

    manifest_path = os.path.join(OUTPUT_DIR, "manifest.json")
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(
            {"source": "wikipedia vi", "items": [r["title"] for r in results]},
            f,
            ensure_ascii=False,
            indent=2,
        )
    print(f"ðŸ“¦ Manifest saved to {manifest_path}")


def parse_args():
    p = argparse.ArgumentParser(description="Wikipedia VI football crawler (BFS)")
    p.add_argument("--max-depth", type=int, default=DEFAULT_MAX_DEPTH)
    p.add_argument("--max-pages", type=int, default=DEFAULT_MAX_PAGES)
    p.add_argument("--delay", type=float, default=DEFAULT_DELAY)
    p.add_argument(
        "--full", action="store_true", help="Táº£i toÃ n bá»™ extract thay vÃ¬ chá»‰ intro"
    )
    p.add_argument(
        "--mode", choices=["extract", "wikitext", "html"], default=DEFAULT_MODE
    )
    return p.parse_args()


def main():
    args = parse_args()
    run(
        max_depth=args.max_depth,
        max_pages=args.max_pages,
        delay=args.delay,
        full=args.full,
        mode=args.mode,
    )


if __name__ == "__main__":
    main()
